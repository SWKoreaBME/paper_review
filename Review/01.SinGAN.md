# SinGan

## Introduction

- 기존 GAN 들은 class-specific 한 model 을 학습하여, 하나의 class 에 대한 여러 이미지들을 training 하여 많은 이미지들에 공통적으로 존재하는 특징을 학습하였다.

  - 이는 pyramid of fully convolutional light-weight GAN 을 사용하여 구현하였다

    ( 각 patch distribution 을 학습하는 여러개의 가벼운 GAN 들이 존재하며, 이들은 전체 이미지를 학습한다기 보다는 각 patch 를 학습하여 생성 )

  - Single training image 의 semantic 을 resemble 하는 high-quality 이미지를 생성해낸다.

- SinGAN 은 하나의 이미지를 학습하는 unconditional generative modeling 이며, 같은 class 의 일반적인 특징을 생성해내는 것 보다는, 하나의 natural image 에서 무조건적인 generation 을 보여준다.

- 가장 연관이 높은 study 는 "**Summarizing Visual Data Using Bidirectional Similarity**" 이며, 여기에는 bidirectional patch similarity measure 와 최적화와 관련된 내용이 적혀있다.

  - 이 work 에 영감을 받아 SinGAN 을 연구할 수 있었음

## Method

![image-20191201015112959](/Users/sangwook/Library/Application Support/typora-user-images/image-20191201015112959.png)

- Goal is to learn **Unconditinal generative modeling** 

  - Capturing the **internal statistics of a single training image x**

  - SinGAN setting is **conceptually similar to the conventional GAN setting**

  - The **training samples are patches of a single image**

  - Going beyond **texture Generation** + Dealing with **more general images**

    - Requires capturing the statistics of complex image structures 

    - from Large objects to fine details & texture information

      => 대충 하나의 이미지에서 patch 의 갯수대로 training sample 을 구성하고, 그것들을 기존에 존재하는 GAN 과 비슷하게 학습시켜서, 큰 특징부터 작은 특징까지 잡아낸다는 뜻

    - **Figure 4** 에 설명이 되어있다 ( **Network 에 대한 설명** )

      - Consists of a hierarchy of patch-GANs (**Markovian Discriminator**)
      - Those GANs have **small receptive fields** and **limited capacity** preventing them from memorizing the single image

  ### Multi Scale Architecture

  - Model consists of a pyramid of generators

    - x0 ~ xN --> G0 ~ GN --> D0 ~ DN

  - 이미지 **sample generation 은 가장 작은 사이즈의 샘플부터 시작** (그리고 차례대로 generator 를 통과시켜 finest scale 까지 올린다. 각 scale 에는 noise 가 추가된다 )

    - 모든 generator 와 discriminator 는 같은 receptive field 를 갖고 있어서 generation process 를 거듭 진행할수록 capture 하는 크기는 줄어든다

    - <img src="/Users/sangwook/Library/Application Support/typora-user-images/image-20191201002603884.png" alt="image-20191201002603884" style="zoom:50%;" />

    - 가장 작은 scale 일때는 purely generative 한 generation 이 진행된다 ? ( 이해 못함 )

      - 첫번째 단계의 receptive field 는 전체 이미지의 약 1/2 정도 되며, 따라서 GN 이 이미지의 general layout (전반적인 영상의 구조?), object's global structure 를 generate 한다.

      - 각 finer scale ( n < N ) 의  generator 들이 이전 scale 에서는 생성하지 않는 detail 들을 더해나간다.

      - Spatial Noise (zn) 의 관점에서 보면, 각 generator (Gn) 는 이전 scale 의 이미지를 upsample 해서 받은 영상을 accept 한다 ( upsampled 된 영상에 noise 를 추가하게 되는데, 이는 convolutional layer 를 통과하기 전에 추가한다 => 이를 통해 convolutional layer 들은 기존 이미지를 그대로 학습하는 것이 아니라, noise 를 추가하게 된 이미지를 학습하게 된다 !)

      - <img src="/Users/sangwook/Library/Application Support/typora-user-images/image-20191201003346787.png" alt="image-20191201003346787" style="zoom:50%;" />

      - Convolutional layer 의 역할은 더 작은 patch 에서 generated 된 이미지에서 놓친 detail 를 generate 하는데에 있다

      - ![image-20191201015134673](/Users/sangwook/Library/Application Support/typora-user-images/image-20191201015134673.png)

        **Details of fully convolutional layer**

  ### Training

  - **가장 작은 scale 부터 순차적으로 학습한다**

  - **n 번째 GAN 의 training loss 는 다음처럼 구성되어 있다**

    **Adversarial term + Reconstruction term**

    ( 다른 GAN 과 마찬가지로, n 번째 Discriminator 의 loss 를 최대화 하고, Gn 의 loss 를 최소화 하는 minmax loss 를 채택하고 있다 )

    ![image-20191201004015437](/Users/sangwook/Library/Application Support/typora-user-images/image-20191201004015437.png)

    - **L**_adv : n 번째 patch 와 생성된 x distribution 의 distance 에 따라 penalize  

    - **L**_rec : insures the existence of a specific set of noise maps that can produce x^n, 이미지 생성을 위해 중요한 feature 이다

    #### Adversarial Loss

    - **WGAN-GP Loss :** 마지막 discrimination score 가 전체 patch 의 discrimination map 들의 평균
    - **[ Contribution ] **기존의 single-image GAN ( for texture ) 과 다른점은 기존에는 random crop 한 영상과의 loss 를 구했는데 지금은 whole image 에 대한 loss 를 구해서 boundary conditions 에 대한 학습이 가능하게 하였다 

    #### Reconstruction Loss

    - noise map set 이 존재하고 ( Gaussian noise ), 이 noise 들로부터 original image x 를 생성하도록 만든다.

      ![image-20191201005117486](/Users/sangwook/Library/Application Support/typora-user-images/image-20191201005117486.png)

    - Reconstructed image 의 또다른 역할은 각 scale 에서 input 으로 들어오는 zn 의 std 를 결정하는 것이다. ( n 번째 std 의 결정은 더 작은 scale (n+1) 에서 생성된 이미지를 upsample 한 것과 n 번째 patch 와의 RMSE (root mean squared error) 에 비례한다 )

      ==> 이것이 의미하는 바는 **더 작은 patch 와의 차이만큼 detail 을 더 추가**하라! 이런 뜻이다.

## Results

- 정성, 정량 분석을 모두 시행
- BSD ( Berkeley Segmentation Database ) 의 이미지를 갖고 test 했으며 세부적인 size 조정을 진행하였다

#### Qualitative Examples

- receptive field 의 사이즈가 원본 이미지보다 작기 때문에 ~ patch 의 다양한 조합으로 기존 이미지와 다른 이미지들을 생성할 수 있음

  #### Effects of scales at test time

  - Multi-scale sized patch 로 나눠 학습하는 것은 sample 사이에 다양성을 제공
  - (당연하지만) 가장 작은 scale 부터 generation 을 진행하는 것이 가장 많은 variability 를 보였다

  #### Effects of scales during training

  <img src="/Users/sangwook/Library/Application Support/typora-user-images/image-20191201015220197.png" alt="image-20191201015220197" style="zoom:50%;" />

  - scale 의 수가 낮으면, fine texture 의 특징만을 잡는다 ( receptive field size 가 크기 때문에 )
  - scale 의 수가 높으면, global object arrangement 가 더 잘 보존된다. 



#### Quantitative Evaluation ( 정량평가 )

이 모델에서 생성된 이미지를 평가하기 위해 두 가지 metrics 를 사용

(1) Amazon Mechanical Turk ( AMT ) - Real/Fake user study

(2) New single-image version of the Frechet Inception Distance

##### AMT Perceptual Study ( further reading )

##### Single Image Frechet Inception Distance ( further reading )



## Applications

Inference 시에는 구조적 변화나 model 의 tuning 을 진행하지 않는다 ( 이미 trained 된 모델 사용 )

=> SinGAN 은 하나의 training image 의 같은 patch distribution 만을 학습한다

- Generation 은  n scale 의 generation pyramid 로 downsampled image 를 넣어서 feed forwarding 한다. 이 때, injection scale 의 변화가 다른 영향을 준다.

#### Super-Resolution

<img src="/Users/sangwook/Library/Application Support/typora-user-images/image-20191201015914798.png" alt="image-20191201015914798" style="zoom:50%;" />

특정 요소나 전체 이미지의 해상도를 높이는 것

- Low resolution 이미지를 학습하고, test 할 때는 r 만큼 rescale 을 진행하고, G0 ( 마지막 generator : 가장 detailed generation 을 하는 .. ) 에 noise 와 함께 inject 한다. => 이 작업을 k 회 반복하여 highness output 을 얻는다.

#### Paint-to-Image

아래 그림처럼 간단한 그림을 통해 원래 이미지와 같은 내용을 갖는 이미지를 생성

![image-20191201020009565](/Users/sangwook/Library/Application Support/typora-user-images/image-20191201020009565.png)

- clipart image ( 대충 그린것 ) 을 downsampling 한 후에 coarse 한 scale 의 generator 에 넣는다. 이렇게 되면 global 한 structure ( 원래 이미지와 matching 되는 ) 는 유지되면서 그 structure 에 해당하는 detail 한 texture 가 generation 된다.

#### Harmonization

<img src="/Users/sangwook/Library/Application Support/typora-user-images/image-20191201012739682.png" alt="image-20191201012739682" style="zoom:50%;" />

#### Editing

<img src="/Users/sangwook/Library/Application Support/typora-user-images/image-20191201012719510.png" alt="image-20191201012719510" style="zoom:50%;" />

#### Single Image Animation

영상 내부에 variation 을 주고, 그것들을 연속적으로 보여주면서 animation 이 가능하다

( Random walk in zspace)

https://www.youtube.com/watch?v=xk8bWLZk4DU&feature=youtu.be



## Conclusion

- SinGAN 은 Unconditianl generative model 이라는 측면에서 좋다
  - 특히, texture 를 학습하는 것이 아니라 하나의 natural image 를 생성해낸다는 점에서 장점
- 하지만,
  - Internal Learning 은 Semantic diversity limitation 을 보인다
    - 예를 들어 한 종류의 강아지가 있다고 가정했을 때, 또 다른 종류의 강아지를 생성하는 것을 불가능하다
- 그럼에도 불구하고, **한 장을 이용해서 generation** 하는 것은 powerful tool 이 될 것이다.











